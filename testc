import requests
import pandas as pd
from bs4 import BeautifulSoup
from requests.auth import HTTPBasicAuth

# Configuration
CONFLUENCE_URL = "https://your-domain.atlassian.net/wiki/rest/api/content"
PAGE_ID = "123456"  # Replace with your page ID
USERNAME = "your-email@example.com"
API_TOKEN = "your-api-token"

def get_confluence_page_content(page_id):
    url = f"{CONFLUENCE_URL}/{page_id}?expand=body.storage"
    response = requests.get(url, auth=HTTPBasicAuth(USERNAME, API_TOKEN))
    
    if response.status_code == 200:
        page_content = response.json()
        html_content = page_content['body']['storage']['value']
        return html_content
    else:
        print(f"Failed to retrieve page content. Status code: {response.status_code}")
        return None

def extract_tables_from_html(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    tables = soup.find_all('table')
    
    extracted_tables = []
    for table in tables:
        rows = []
        for row in table.find_all('tr'):
            cols = [col.get_text(strip=True) for col in row.find_all(['td', 'th'])]
            rows.append(cols)
        df = pd.DataFrame(rows)
        extracted_tables.append(df)
    return extracted_tables

def main():
    html_content = get_confluence_page_content(PAGE_ID)
    if html_content:
        tables = extract_tables_from_html(html_content)
        
        # Save each table to a different sheet in an Excel file
        with pd.ExcelWriter("confluence_tables.xlsx", engine="openpyxl") as writer:
            for i, table in enumerate(tables, start=1):
                sheet_name = f"Table_{i}"
                table.to_excel(writer, sheet_name=sheet_name, index=False, header=False)
                print(f"Table {i} saved in sheet '{sheet_name}'")

if __name__ == "__main__":
    main()
